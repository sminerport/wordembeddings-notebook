{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"wordEmbeddings.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","mount_file_id":"1StIBevO1Ip-xSZEZkDCLXwU-PvGwXXt_","authorship_tag":"ABX9TyPViWIVRFss6DuiUrwP/I7c"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"r4ephzhBKM68","executionInfo":{"status":"ok","timestamp":1633064106887,"user_tz":360,"elapsed":82846,"user":{"displayName":"Scott Miner","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhxCt3KNHFc4ZlPw4tBG4aCXpKie347LZ9L1JAVuA=s64","userId":"08363991975257577328"}},"outputId":"71da2454-8871-4d52-cba2-e815fd5ce360"},"source":["# import libraries\n","import pandas as pd\n","import numpy as np\n","import os\n","\n","TRAIN_CSV = 'Train.csv'\n","VALID_CSV = 'Valid.csv'\n","MODEL_NAME = 'best_model.h5'\n","data_path = '/content/drive/MyDrive/Colab Notebooks/chatbot-flask-simple/data/movie_reviews'\n","\n","training = os.path.join(data_path, TRAIN_CSV)\n","validation = os.path.join(data_path, VALID_CSV)\n","model_path = os.path.join(data_path, MODEL_NAME)\n","# reading csv files\n","train = pd.read_csv(training)\n","valid = pd.read_csv(validation)\n","\n","#train_test split\n","x_tr, y_tr = train['text'].values, train['label'].values\n","x_val, y_val = valid['text'].values, valid['label'].values\n","\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","\n","#Tokenize the sentences\n","tokenizer = Tokenizer()\n","#preparing vocabulary\n","tokenizer.fit_on_texts(list(x_tr))\n","\n","# converting text into integer sequences\n","x_tr_seq = tokenizer.texts_to_sequences(x_tr)\n","x_val_seq = tokenizer.texts_to_sequences(x_val)\n","\n","# padding to prepare sequences of same length\n","x_tr_seq = pad_sequences(x_tr_seq, maxlen=100)\n","x_val_seq = pad_sequences(x_val_seq, maxlen=100)\n","\n","size_of_vocabulary = len(tokenizer.word_index) + 1 #+1 for padding\n","print(size_of_vocabulary)\n","\n","# build two different NLP models of the same architecture.  The first learns\n","# embeddings from scratch the second uses pretrained word embeddings\n","from keras.models import *\n","from keras.layers import *\n","from keras.callbacks import *\n","\n","model = Sequential()\n","\n","#embedding layer\n","model.add(Embedding(size_of_vocabulary,300,input_length=100,trainable=True))\n","\n","#lstm layer\n","model.add(LSTM(128,return_sequences=True,dropout=0.2))\n","\n","#Global Maxpooling\n","model.add(GlobalMaxPooling1D())\n","\n","#Dense Layer\n","model.add(Dense(64,activation='relu'))\n","model.add(Dense(1,activation='sigmoid'))\n","\n","#Add loss function, metrics, optimizer\n","model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['acc'])\n","\n","#addingcallbacks\n","es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=)\n","mc = ModelCheckpoint(model_path, monitor='val_acc', mode='max', \n","                     save_best_only=True, verbose=1)\n","\n","#Print summary of model\n","print(model.summary())\n","\n","history = model.fit(np.array(x_tr_seq),np.array(y_tr),batch_size=128,epochs=10,\n","                    validation_data=(np.array(x_val_seq),np.array(y_val)),\n","                    verbose=1,callbacks=[es,mc])\n","\n","#loading best model\n","from keras.models import load_model\n","model = load_model(model_path)\n","\n","# evaluation\n","_,val_acc = model.evaluate(x_val_seq,y_val,batch_size=128)\n","print(val_acc)"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["112204\n","Model: \"sequential_1\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_1 (Embedding)      (None, 100, 300)          33661200  \n","_________________________________________________________________\n","lstm_1 (LSTM)                (None, 100, 128)          219648    \n","_________________________________________________________________\n","global_max_pooling1d_1 (Glob (None, 128)               0         \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 64)                8256      \n","_________________________________________________________________\n","dense_3 (Dense)              (None, 1)                 65        \n","=================================================================\n","Total params: 33,889,169\n","Trainable params: 33,889,169\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n","Epoch 1/10\n","313/313 [==============================] - 8s 21ms/step - loss: 0.4065 - acc: 0.8058 - val_loss: 0.3594 - val_acc: 0.8440\n","\n","Epoch 00001: val_acc improved from -inf to 0.84400, saving model to /content/drive/MyDrive/Colab Notebooks/chatbot-flask-simple/data/movie_reviews/best_model.h5\n","Epoch 2/10\n","313/313 [==============================] - 6s 20ms/step - loss: 0.2087 - acc: 0.9190 - val_loss: 0.3208 - val_acc: 0.8662\n","\n","Epoch 00002: val_acc improved from 0.84400 to 0.86620, saving model to /content/drive/MyDrive/Colab Notebooks/chatbot-flask-simple/data/movie_reviews/best_model.h5\n","Epoch 3/10\n","313/313 [==============================] - 6s 19ms/step - loss: 0.1031 - acc: 0.9632 - val_loss: 0.4417 - val_acc: 0.8524\n","\n","Epoch 00003: val_acc did not improve from 0.86620\n","Epoch 4/10\n","313/313 [==============================] - 6s 20ms/step - loss: 0.0505 - acc: 0.9826 - val_loss: 0.5493 - val_acc: 0.8506\n","\n","Epoch 00004: val_acc did not improve from 0.86620\n","Epoch 5/10\n","313/313 [==============================] - 6s 19ms/step - loss: 0.0238 - acc: 0.9923 - val_loss: 0.6004 - val_acc: 0.8546\n","\n","Epoch 00005: val_acc did not improve from 0.86620\n","Epoch 6/10\n","313/313 [==============================] - 6s 19ms/step - loss: 0.0130 - acc: 0.9962 - val_loss: 0.8000 - val_acc: 0.8498\n","\n","Epoch 00006: val_acc did not improve from 0.86620\n","Epoch 7/10\n","313/313 [==============================] - 6s 19ms/step - loss: 0.0117 - acc: 0.9959 - val_loss: 0.9311 - val_acc: 0.8454\n","\n","Epoch 00007: val_acc did not improve from 0.86620\n","Epoch 8/10\n","313/313 [==============================] - 6s 19ms/step - loss: 0.0091 - acc: 0.9968 - val_loss: 0.9382 - val_acc: 0.8460\n","\n","Epoch 00008: val_acc did not improve from 0.86620\n","Epoch 9/10\n","313/313 [==============================] - 6s 19ms/step - loss: 0.0111 - acc: 0.9962 - val_loss: 0.9347 - val_acc: 0.8446\n","\n","Epoch 00009: val_acc did not improve from 0.86620\n","Epoch 10/10\n","313/313 [==============================] - 6s 19ms/step - loss: 0.0075 - acc: 0.9976 - val_loss: 0.9944 - val_acc: 0.8424\n","\n","Epoch 00010: val_acc did not improve from 0.86620\n","40/40 [==============================] - 0s 5ms/step - loss: 0.3208 - acc: 0.8662\n","0.8661999702453613\n"]}]},{"cell_type":"markdown","metadata":{"id":"TaRYpqS_XgLX"},"source":["Build version II using GloVe pretrained word embeddings. Let's load the GloVe embeddings into our environment. "]},{"cell_type":"code","metadata":{"id":"KIiyDx1actOk"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tdRSoUEoXd64"},"source":["# load the whole embedding into memory\n","embeddings_index = dict()\n","f = open"],"execution_count":null,"outputs":[]}]}