{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"wordEmbeddings.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","mount_file_id":"1StIBevO1Ip-xSZEZkDCLXwU-PvGwXXt_","authorship_tag":"ABX9TyPQ0emE4P2YtAOgLrKBAF9L"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"collapsed":true,"id":"r4ephzhBKM68","executionInfo":{"status":"ok","timestamp":1633118745900,"user_tz":360,"elapsed":304,"user":{"displayName":"Scott Miner","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhxCt3KNHFc4ZlPw4tBG4aCXpKie347LZ9L1JAVuA=s64","userId":"08363991975257577328"}}},"source":["# import libraries\n","import pandas as pd\n","import numpy as np\n","import os\n","\n","TRAIN_CSV = 'Train.csv'\n","VALID_CSV = 'Valid.csv'\n","data_path = '/content/drive/MyDrive/Colab Notebooks/wordEmbeddings/data/movie_reviews'\n","\n","MODEL_NAME1 = 'best_model_scratch.h5'\n","MODEL_NAME2 = 'best_model_pretrained.h5'\n","model_path = '/content/drive/MyDrive/Colab Notebooks/wordEmbeddings/model'\n","\n","training = os.path.join(data_path, TRAIN_CSV)\n","validation = os.path.join(data_path, VALID_CSV)\n","model_path_scratch = os.path.join(model_path, MODEL_NAME1)\n","model_path_pretrained = os.path.join(model_path, MODEL_NAME2)\n"],"execution_count":70,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0dBdhecNb7Uu","executionInfo":{"status":"ok","timestamp":1633118798757,"user_tz":360,"elapsed":48189,"user":{"displayName":"Scott Miner","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhxCt3KNHFc4ZlPw4tBG4aCXpKie347LZ9L1JAVuA=s64","userId":"08363991975257577328"}},"outputId":"2b1b0116-f129-423b-bb53-a9be763d8461"},"source":["\n","# reading csv files\n","train = pd.read_csv(training)\n","valid = pd.read_csv(validation)\n","\n","#train_test split\n","x_tr, y_tr = train['text'].values, train['label'].values\n","x_val, y_val = valid['text'].values, valid['label'].values\n","\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","\n","#Tokenize the sentences\n","tokenizer = Tokenizer()\n","#preparing vocabulary\n","tokenizer.fit_on_texts(list(x_tr))\n","\n","# converting text into integer sequences\n","x_tr_seq = tokenizer.texts_to_sequences(x_tr)\n","x_val_seq = tokenizer.texts_to_sequences(x_val)\n","\n","# padding to prepare sequences of same length\n","x_tr_seq = pad_sequences(x_tr_seq, maxlen=100)\n","x_val_seq = pad_sequences(x_val_seq, maxlen=100)\n","\n","size_of_vocabulary = len(tokenizer.word_index) + 1 #+1 for padding\n","print(f'Size of vocab: {size_of_vocabulary}')\n","\n","# build two different NLP models of the same architecture.  The first learns\n","# embeddings from scratch the second uses pretrained word embeddings\n","from keras.models import *\n","from keras.layers import *\n","from keras.callbacks import *\n","\n","model = Sequential()\n","\n","#embedding layer\n","model.add(Embedding(size_of_vocabulary,300,input_length=100,trainable=True))\n","\n","#lstm layer\n","model.add(LSTM(128,return_sequences=True,dropout=0.2))\n","\n","#Global Maxpooling\n","model.add(GlobalMaxPooling1D())\n","\n","#Dense Layer\n","model.add(Dense(64,activation='relu'))\n","model.add(Dense(1,activation='sigmoid'))\n","\n","#Add loss function, metrics, optimizer\n","model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['acc'])\n","\n","#addingcallbacks\n","es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=3)\n","mc = ModelCheckpoint(model_path_scratch, monitor='val_acc', mode='max', \n","                     save_best_only=True, verbose=1)\n","\n","#Print summary of model\n","print(model.summary())\n","\n","history = model.fit(np.array(x_tr_seq),np.array(y_tr),batch_size=128,epochs=10,\n","                    validation_data=(np.array(x_val_seq),np.array(y_val)),\n","                    verbose=1,callbacks=[es,mc])\n"],"execution_count":71,"outputs":[{"output_type":"stream","name":"stdout","text":["Size of vocab: 112204\n","Model: \"sequential_8\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_8 (Embedding)      (None, 100, 300)          33661200  \n","_________________________________________________________________\n","lstm_8 (LSTM)                (None, 100, 128)          219648    \n","_________________________________________________________________\n","global_max_pooling1d_8 (Glob (None, 128)               0         \n","_________________________________________________________________\n","dense_16 (Dense)             (None, 64)                8256      \n","_________________________________________________________________\n","dense_17 (Dense)             (None, 1)                 65        \n","=================================================================\n","Total params: 33,889,169\n","Trainable params: 33,889,169\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n","Epoch 1/10\n","313/313 [==============================] - 9s 22ms/step - loss: 0.4003 - acc: 0.8140 - val_loss: 0.3158 - val_acc: 0.8676\n","\n","Epoch 00001: val_acc improved from -inf to 0.86760, saving model to /content/drive/MyDrive/Colab Notebooks/wordEmbeddings/model/best_model_scratch.h5\n","Epoch 2/10\n","313/313 [==============================] - 6s 20ms/step - loss: 0.2030 - acc: 0.9204 - val_loss: 0.3213 - val_acc: 0.8738\n","\n","Epoch 00002: val_acc improved from 0.86760 to 0.87380, saving model to /content/drive/MyDrive/Colab Notebooks/wordEmbeddings/model/best_model_scratch.h5\n","Epoch 3/10\n","313/313 [==============================] - 6s 20ms/step - loss: 0.1027 - acc: 0.9631 - val_loss: 0.3726 - val_acc: 0.8594\n","\n","Epoch 00003: val_acc did not improve from 0.87380\n","Epoch 4/10\n","313/313 [==============================] - 6s 20ms/step - loss: 0.0468 - acc: 0.9849 - val_loss: 0.5498 - val_acc: 0.8544\n","\n","Epoch 00004: val_acc did not improve from 0.87380\n","Epoch 00004: early stopping\n","40/40 [==============================] - 1s 5ms/step - loss: 0.3213 - acc: 0.8738\n","0.8737999796867371\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EXpvzjeierZ7","executionInfo":{"status":"ok","timestamp":1633119034136,"user_tz":360,"elapsed":5179,"user":{"displayName":"Scott Miner","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhxCt3KNHFc4ZlPw4tBG4aCXpKie347LZ9L1JAVuA=s64","userId":"08363991975257577328"}},"outputId":"a275a08f-5981-49c0-b22b-a5e28601f6a8"},"source":["\n","#loading best model\n","from keras.models import load_model\n","model = load_model(model_path_scratch)\n","\n","# evaluation\n","_,val_acc = model.evaluate(x_val_seq,y_val,batch_size=128)\n","print(val_acc)"],"execution_count":78,"outputs":[{"output_type":"stream","name":"stdout","text":["40/40 [==============================] - 1s 6ms/step - loss: 0.3213 - acc: 0.8738\n","0.8737999796867371\n"]}]},{"cell_type":"markdown","metadata":{"id":"TaRYpqS_XgLX"},"source":["Build version II using GloVe pretrained word embeddings. Let's load the GloVe embeddings into our environment. "]},{"cell_type":"markdown","metadata":{"id":"Y_QWZspi8Z8z"},"source":["# Download file."]},{"cell_type":"code","metadata":{"id":"KIiyDx1actOk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1633116391349,"user_tz":360,"elapsed":302,"user":{"displayName":"Scott Miner","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhxCt3KNHFc4ZlPw4tBG4aCXpKie347LZ9L1JAVuA=s64","userId":"08363991975257577328"}},"outputId":"ead3793e-4c5b-404d-f0cf-c6f2b3af359e"},"source":["import os\n","import tqdm\n","import requests\n","import zipfile\n","\n","URL = 'http://nlp.stanford.edu/data/glove.840B.300d.zip'\n","\n","def fetch_data(url=URL, target_file='/content/drive/MyDrive/Colab Notebooks/wordEmbeddings/embeddings/glove.zip', delete_zip=False):\n","    # if dataset exists exit\n","    if os.path.isfile(target_file):\n","        print('datasets already downloaded')\n","        return\n","\n","        #download (large) zip file\n","    #for large https request on stream mode to avoid out of memory issues\n","    #see : http://masnun.com/2016/09/18/python-using-the-requests-module-to-download-large-files-efficiently.html\n","    print(\"**************************\")\n","    print(\"  Downloading zip file\")\n","    print(\"  >_<  Please wait >_< \")\n","    print(\"**************************\")\n","    response = requests.get(url, stream=True)\n","    #read chunk by chunk\n","    handle = open(target_file, \"wb\")\n","    for chunk in tqdm.tqdm(response.iter_content(chunk_size=512)):\n","        if chunk:  \n","            handle.write(chunk)\n","    handle.close()  \n","    print(\"  Download completed ;) :\") \n","    #extract zip_file\n","    zf = zipfile.ZipFile(target_file)\n","    print(\"1. Extracting {} file\".format(target_file))\n","    zf.extractall(path='/content/drive/MyDrive/Colab Notebooks/wordEmbeddings/embeddings')\n","    if delete_zip:\n","        print(\"2. Deleting {} file\".format(dataset_name+\".zip\"))\n","        os.remove(path=zip_file)\n","\n","fetch_data()"],"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["datasets already downloaded\n"]}]},{"cell_type":"markdown","metadata":{"id":"oRKxg6eDDIkx"},"source":["# Construct an embedding matrix"]},{"cell_type":"markdown","metadata":{"id":"Z1zl52HyD8Bx"},"source":["The embedding matrix maps each word index to its corresponding embedding vector.\n","<img src=\"https://drive.google.com/uc?id=19lmu8VSTlAdWl_-YcHtZwivRIx2rd57n&authuser=scottminer1205%40gmail.com&usp=drive_fs\" width=600/>\n"]},{"cell_type":"markdown","metadata":{"id":"ybkqVFPUeBFv"},"source":["# Load embedding into memory\n","\n","Takes some time to run."]},{"cell_type":"code","metadata":{"id":"tdRSoUEoXd64","colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"status":"ok","timestamp":1633118945929,"user_tz":360,"elapsed":105735,"user":{"displayName":"Scott Miner","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhxCt3KNHFc4ZlPw4tBG4aCXpKie347LZ9L1JAVuA=s64","userId":"08363991975257577328"}},"outputId":"15aac818-20b9-4def-ec94-f7578488741d"},"source":["# load the whole embedding into memory\n","path_to_glove_file = '/content/drive/MyDrive/Colab Notebooks/wordEmbeddings/embeddings/glove.840B.300d.txt'\n","\n","embeddings_index = {}\n","with open(path_to_glove_file) as f:\n","    for line in f:\n","        word, coefs = line.split(maxsplit=1)\n","        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n","        embeddings_index[word] = coefs\n","\n","print(\"Found %s word vectors.\" % len(embeddings_index))\n"],"execution_count":74,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: DeprecationWarning: string or file could not be read to its end due to unmatched data; this will raise a ValueError in the future.\n","  \n"]},{"output_type":"stream","name":"stdout","text":["Found 2195884 word vectors.\n"]}]},{"cell_type":"markdown","metadata":{"id":"X0G-SSN3KLKx"},"source":["Let's create an embedding matrix by assigning the vocabulary with the pretrained word embeddings."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XUnhm2oeGSTd","executionInfo":{"status":"ok","timestamp":1633118967794,"user_tz":360,"elapsed":679,"user":{"displayName":"Scott Miner","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhxCt3KNHFc4ZlPw4tBG4aCXpKie347LZ9L1JAVuA=s64","userId":"08363991975257577328"}},"outputId":"27231495-882b-4cec-987c-bafb634fb237"},"source":["\n","# create a weight matrix for words in training docs\n","embedding_matrix = np.zeros((size_of_vocabulary, 300))\n","hits = 0\n","misses = 0\n","\n","for word, i in tokenizer.word_index.items():\n","    embedding_vector = embeddings_index.get(word)\n","    if embedding_vector is not None and embedding_vector.shape[0] != 0:       \n","        embedding_matrix[i] = embedding_vector\n","        hits += 1\n","    else:\n","        misses += 1\n","print(f'Converted {hits} words ({misses} misses)')       \n"],"execution_count":75,"outputs":[{"output_type":"stream","name":"stdout","text":["Converted 73041 words (39162 misses)\n"]}]},{"cell_type":"markdown","metadata":{"id":"6-RCwF59ajX4"},"source":["# Defining the model architecture - pretrained embeddings:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Jw4z5DKVUDXv","executionInfo":{"status":"ok","timestamp":1633119241129,"user_tz":360,"elapsed":600,"user":{"displayName":"Scott Miner","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhxCt3KNHFc4ZlPw4tBG4aCXpKie347LZ9L1JAVuA=s64","userId":"08363991975257577328"}},"outputId":"3957aacb-6278-45be-8faa-a93921b745f8"},"source":["model = Sequential()\n","\n","#embedding layer\n","model.add(Embedding(size_of_vocabulary,300,\n","                    weights=[embedding_matrix],\n","                    input_length=100,trainable=False))\n","\n","#lstm layer\n","model.add(LSTM(128,return_sequences=True,dropout=0.2))\n","\n","#Global Maxpooling\n","model.add(GlobalMaxPooling1D())\n","\n","model.add(Dense(64,activation='relu'))\n","model.add(Dense(1,activation='sigmoid'))\n","\n","# add loss, metrics, optimizer\n","model.compile(optimizer='adam', loss='binary_crossentropy',metrics=['acc'])\n","\n","# adding callbacks\n","es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=3)\n","mc = ModelCheckpoint(model_path_pretrained, monitor='val_acc', mode='max', \n","                     save_best_only=True,verbose=1)\n","\n","#print summary of model\n","print(model.summary())\n","\n"],"execution_count":80,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_10\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_10 (Embedding)     (None, 100, 300)          33661200  \n","_________________________________________________________________\n","lstm_10 (LSTM)               (None, 100, 128)          219648    \n","_________________________________________________________________\n","global_max_pooling1d_10 (Glo (None, 128)               0         \n","_________________________________________________________________\n","dense_20 (Dense)             (None, 64)                8256      \n","_________________________________________________________________\n","dense_21 (Dense)             (None, 1)                 65        \n","=================================================================\n","Total params: 33,889,169\n","Trainable params: 227,969\n","Non-trainable params: 33,661,200\n","_________________________________________________________________\n","None\n"]}]},{"cell_type":"markdown","metadata":{"id":"p_cst8Hue3_f"},"source":["# Train the model"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pLt6CBhne7bp","executionInfo":{"status":"ok","timestamp":1633119286929,"user_tz":360,"elapsed":40598,"user":{"displayName":"Scott Miner","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhxCt3KNHFc4ZlPw4tBG4aCXpKie347LZ9L1JAVuA=s64","userId":"08363991975257577328"}},"outputId":"da468036-2438-4cfa-9d26-b22d34193ae7"},"source":["history = model.fit(np.array(x_tr_seq),np.array(y_tr),batch_size=128,\n","                    epochs=10,validation_data=(np.array(x_val_seq),\n","                                               np.array(y_val)),verbose=1,\n","                                               callbacks=[es,mc])"],"execution_count":81,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","313/313 [==============================] - 6s 13ms/step - loss: 0.4234 - acc: 0.7972 - val_loss: 0.3308 - val_acc: 0.8552\n","\n","Epoch 00001: val_acc improved from -inf to 0.85520, saving model to /content/drive/MyDrive/Colab Notebooks/wordEmbeddings/model/best_model_pretrained.h5\n","Epoch 2/10\n","313/313 [==============================] - 4s 12ms/step - loss: 0.3268 - acc: 0.8550 - val_loss: 0.3045 - val_acc: 0.8666\n","\n","Epoch 00002: val_acc improved from 0.85520 to 0.86660, saving model to /content/drive/MyDrive/Colab Notebooks/wordEmbeddings/model/best_model_pretrained.h5\n","Epoch 3/10\n","313/313 [==============================] - 4s 12ms/step - loss: 0.2885 - acc: 0.8761 - val_loss: 0.2879 - val_acc: 0.8740\n","\n","Epoch 00003: val_acc improved from 0.86660 to 0.87400, saving model to /content/drive/MyDrive/Colab Notebooks/wordEmbeddings/model/best_model_pretrained.h5\n","Epoch 4/10\n","313/313 [==============================] - 4s 12ms/step - loss: 0.2568 - acc: 0.8928 - val_loss: 0.3001 - val_acc: 0.8710\n","\n","Epoch 00004: val_acc did not improve from 0.87400\n","Epoch 5/10\n","313/313 [==============================] - 4s 12ms/step - loss: 0.2289 - acc: 0.9052 - val_loss: 0.2849 - val_acc: 0.8752\n","\n","Epoch 00005: val_acc improved from 0.87400 to 0.87520, saving model to /content/drive/MyDrive/Colab Notebooks/wordEmbeddings/model/best_model_pretrained.h5\n","Epoch 6/10\n","313/313 [==============================] - 4s 12ms/step - loss: 0.1997 - acc: 0.9195 - val_loss: 0.3097 - val_acc: 0.8736\n","\n","Epoch 00006: val_acc did not improve from 0.87520\n","Epoch 7/10\n","313/313 [==============================] - 4s 12ms/step - loss: 0.1751 - acc: 0.9309 - val_loss: 0.2972 - val_acc: 0.8840\n","\n","Epoch 00007: val_acc improved from 0.87520 to 0.88400, saving model to /content/drive/MyDrive/Colab Notebooks/wordEmbeddings/model/best_model_pretrained.h5\n","Epoch 8/10\n","313/313 [==============================] - 4s 12ms/step - loss: 0.1512 - acc: 0.9399 - val_loss: 0.3079 - val_acc: 0.8810\n","\n","Epoch 00008: val_acc did not improve from 0.88400\n","Epoch 00008: early stopping\n"]}]},{"cell_type":"markdown","metadata":{"id":"NU-9WVaCbMZN"},"source":["# Evaluating model performance"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KZgF3cIlcPBr","executionInfo":{"status":"ok","timestamp":1633119316135,"user_tz":360,"elapsed":3820,"user":{"displayName":"Scott Miner","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhxCt3KNHFc4ZlPw4tBG4aCXpKie347LZ9L1JAVuA=s64","userId":"08363991975257577328"}},"outputId":"e2a23e99-1da9-497d-d5d4-dadafa6a3883"},"source":["# loading best model\n","from keras.models import load_model\n","model = load_model(model_path_pretrained)\n","\n","_, val_acc = model.evaluate(x_val_seq,y_val, batch_size=128)\n","print(val_acc)"],"execution_count":82,"outputs":[{"output_type":"stream","name":"stdout","text":["40/40 [==============================] - 1s 6ms/step - loss: 0.2972 - acc: 0.8840\n","0.8840000033378601\n"]}]},{"cell_type":"code","metadata":{"id":"HiUfW4Twca2o"},"source":[""],"execution_count":null,"outputs":[]}]}