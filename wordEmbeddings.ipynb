{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"wordEmbeddings.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","mount_file_id":"1StIBevO1Ip-xSZEZkDCLXwU-PvGwXXt_","authorship_tag":"ABX9TyP8hgJeKwKn1i2DPL1Dx67t"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"r4ephzhBKM68","executionInfo":{"status":"ok","timestamp":1633116366226,"user_tz":360,"elapsed":62273,"user":{"displayName":"Scott Miner","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhxCt3KNHFc4ZlPw4tBG4aCXpKie347LZ9L1JAVuA=s64","userId":"08363991975257577328"}},"outputId":"e0a74716-06d3-4bd8-d344-c9099c1eda6c"},"source":["# import libraries\n","import pandas as pd\n","import numpy as np\n","import os\n","\n","TRAIN_CSV = 'Train.csv'\n","VALID_CSV = 'Valid.csv'\n","MODEL_NAME = 'best_model.h5'\n","data_path = '/content/drive/MyDrive/Colab Notebooks/wordEmbeddings/data/movie_reviews'\n","\n","training = os.path.join(data_path, TRAIN_CSV)\n","validation = os.path.join(data_path, VALID_CSV)\n","model_path = os.path.join(data_path, MODEL_NAME)\n","# reading csv files\n","train = pd.read_csv(training)\n","valid = pd.read_csv(validation)\n","\n","#train_test split\n","x_tr, y_tr = train['text'].values, train['label'].values\n","x_val, y_val = valid['text'].values, valid['label'].values\n","\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","\n","#Tokenize the sentences\n","tokenizer = Tokenizer()\n","#preparing vocabulary\n","tokenizer.fit_on_texts(list(x_tr))\n","\n","# converting text into integer sequences\n","x_tr_seq = tokenizer.texts_to_sequences(x_tr)\n","x_val_seq = tokenizer.texts_to_sequences(x_val)\n","\n","# padding to prepare sequences of same length\n","x_tr_seq = pad_sequences(x_tr_seq, maxlen=100)\n","x_val_seq = pad_sequences(x_val_seq, maxlen=100)\n","\n","size_of_vocabulary = len(tokenizer.word_index) + 1 #+1 for padding\n","print(f'Size of vocab: {size_of_vocabulary}')\n","\n","# build two different NLP models of the same architecture.  The first learns\n","# embeddings from scratch the second uses pretrained word embeddings\n","from keras.models import *\n","from keras.layers import *\n","from keras.callbacks import *\n","\n","model = Sequential()\n","\n","#embedding layer\n","model.add(Embedding(size_of_vocabulary,300,input_length=100,trainable=True))\n","\n","#lstm layer\n","model.add(LSTM(128,return_sequences=True,dropout=0.2))\n","\n","#Global Maxpooling\n","model.add(GlobalMaxPooling1D())\n","\n","#Dense Layer\n","model.add(Dense(64,activation='relu'))\n","model.add(Dense(1,activation='sigmoid'))\n","\n","#Add loss function, metrics, optimizer\n","model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['acc'])\n","\n","#addingcallbacks\n","es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=3)\n","mc = ModelCheckpoint(model_path, monitor='val_acc', mode='max', \n","                     save_best_only=True, verbose=1)\n","\n","#Print summary of model\n","print(model.summary())\n","\n","history = model.fit(np.array(x_tr_seq),np.array(y_tr),batch_size=128,epochs=10,\n","                    validation_data=(np.array(x_val_seq),np.array(y_val)),\n","                    verbose=1,callbacks=[es,mc])\n","\n","#loading best model\n","from keras.models import load_model\n","model = load_model(model_path)\n","\n","# evaluation\n","_,val_acc = model.evaluate(x_val_seq,y_val,batch_size=128)\n","print(val_acc)"],"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["Size of vocab: 112204\n","Model: \"sequential_2\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_2 (Embedding)      (None, 100, 300)          33661200  \n","_________________________________________________________________\n","lstm_2 (LSTM)                (None, 100, 128)          219648    \n","_________________________________________________________________\n","global_max_pooling1d_2 (Glob (None, 128)               0         \n","_________________________________________________________________\n","dense_4 (Dense)              (None, 64)                8256      \n","_________________________________________________________________\n","dense_5 (Dense)              (None, 1)                 65        \n","=================================================================\n","Total params: 33,889,169\n","Trainable params: 33,889,169\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n","Epoch 1/10\n","313/313 [==============================] - 9s 22ms/step - loss: 0.4034 - acc: 0.8053 - val_loss: 0.3185 - val_acc: 0.8642\n","\n","Epoch 00001: val_acc improved from -inf to 0.86420, saving model to /content/drive/MyDrive/Colab Notebooks/wordEmbeddings/data/movie_reviews/best_model.h5\n","Epoch 2/10\n","313/313 [==============================] - 7s 21ms/step - loss: 0.2060 - acc: 0.9179 - val_loss: 0.3150 - val_acc: 0.8696\n","\n","Epoch 00002: val_acc improved from 0.86420 to 0.86960, saving model to /content/drive/MyDrive/Colab Notebooks/wordEmbeddings/data/movie_reviews/best_model.h5\n","Epoch 3/10\n","313/313 [==============================] - 7s 21ms/step - loss: 0.1034 - acc: 0.9632 - val_loss: 0.3812 - val_acc: 0.8596\n","\n","Epoch 00003: val_acc did not improve from 0.86960\n","Epoch 4/10\n","313/313 [==============================] - 7s 21ms/step - loss: 0.0475 - acc: 0.9846 - val_loss: 0.5557 - val_acc: 0.8546\n","\n","Epoch 00004: val_acc did not improve from 0.86960\n","Epoch 5/10\n","313/313 [==============================] - 6s 21ms/step - loss: 0.0213 - acc: 0.9934 - val_loss: 0.6758 - val_acc: 0.8580\n","\n","Epoch 00005: val_acc did not improve from 0.86960\n","Epoch 00005: early stopping\n","40/40 [==============================] - 1s 5ms/step - loss: 0.3150 - acc: 0.8696\n","0.8695999979972839\n"]}]},{"cell_type":"markdown","metadata":{"id":"TaRYpqS_XgLX"},"source":["Build version II using GloVe pretrained word embeddings. Let's load the GloVe embeddings into our environment. "]},{"cell_type":"markdown","metadata":{"id":"Y_QWZspi8Z8z"},"source":["# Download file."]},{"cell_type":"code","metadata":{"id":"KIiyDx1actOk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1633116391349,"user_tz":360,"elapsed":302,"user":{"displayName":"Scott Miner","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhxCt3KNHFc4ZlPw4tBG4aCXpKie347LZ9L1JAVuA=s64","userId":"08363991975257577328"}},"outputId":"ead3793e-4c5b-404d-f0cf-c6f2b3af359e"},"source":["import os\n","import tqdm\n","import requests\n","import zipfile\n","\n","URL = 'http://nlp.stanford.edu/data/glove.840B.300d.zip'\n","\n","def fetch_data(url=URL, target_file='/content/drive/MyDrive/Colab Notebooks/wordEmbeddings/embeddings/glove.zip', delete_zip=False):\n","    # if dataset exists exit\n","    if os.path.isfile(target_file):\n","        print('datasets already downloaded')\n","        return\n","\n","        #download (large) zip file\n","    #for large https request on stream mode to avoid out of memory issues\n","    #see : http://masnun.com/2016/09/18/python-using-the-requests-module-to-download-large-files-efficiently.html\n","    print(\"**************************\")\n","    print(\"  Downloading zip file\")\n","    print(\"  >_<  Please wait >_< \")\n","    print(\"**************************\")\n","    response = requests.get(url, stream=True)\n","    #read chunk by chunk\n","    handle = open(target_file, \"wb\")\n","    for chunk in tqdm.tqdm(response.iter_content(chunk_size=512)):\n","        if chunk:  \n","            handle.write(chunk)\n","    handle.close()  \n","    print(\"  Download completed ;) :\") \n","    #extract zip_file\n","    zf = zipfile.ZipFile(target_file)\n","    print(\"1. Extracting {} file\".format(target_file))\n","    zf.extractall(path='/content/drive/MyDrive/Colab Notebooks/wordEmbeddings/embeddings')\n","    if delete_zip:\n","        print(\"2. Deleting {} file\".format(dataset_name+\".zip\"))\n","        os.remove(path=zip_file)\n","\n","fetch_data()"],"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["datasets already downloaded\n"]}]},{"cell_type":"markdown","metadata":{"id":"oRKxg6eDDIkx"},"source":["# Construct an embedding matrix"]},{"cell_type":"markdown","metadata":{"id":"Z1zl52HyD8Bx"},"source":["The embedding matrix maps each word index to its corresponding embedding vector.\n","<img src=\"https://drive.google.com/uc?id=19lmu8VSTlAdWl_-YcHtZwivRIx2rd57n&authuser=scottminer1205%40gmail.com&usp=drive_fs\" width=600/>\n"]},{"cell_type":"code","metadata":{"id":"tdRSoUEoXd64","colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"status":"ok","timestamp":1633116508084,"user_tz":360,"elapsed":107591,"user":{"displayName":"Scott Miner","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhxCt3KNHFc4ZlPw4tBG4aCXpKie347LZ9L1JAVuA=s64","userId":"08363991975257577328"}},"outputId":"9e2d75f1-b2ec-4842-ab44-c87af0d9ec8f"},"source":["# load the whole embedding into memory\n","path_to_glove_file = '/content/drive/MyDrive/Colab Notebooks/wordEmbeddings/embeddings/glove.840B.300d.txt'\n","\n","embeddings_index = {}\n","with open(path_to_glove_file) as f:\n","    for line in f:\n","        word, coefs = line.split(maxsplit=1)\n","        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n","        embeddings_index[word] = coefs\n","\n","print(\"Found %s word vectors.\" % len(embeddings_index))\n"],"execution_count":25,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: DeprecationWarning: string or file could not be read to its end due to unmatched data; this will raise a ValueError in the future.\n","  \n"]},{"output_type":"stream","name":"stdout","text":["Found 2195884 word vectors.\n"]}]},{"cell_type":"markdown","metadata":{"id":"X0G-SSN3KLKx"},"source":["Let's create an embedding matrix by assigning the vocabulary with the pretrained word embeddings."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":183},"id":"mVOqcLnTV-al","executionInfo":{"status":"error","timestamp":1633117518609,"user_tz":360,"elapsed":373,"user":{"displayName":"Scott Miner","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhxCt3KNHFc4ZlPw4tBG4aCXpKie347LZ9L1JAVuA=s64","userId":"08363991975257577328"}},"outputId":"4427957e-06b3-43c7-8406-93c50b9fc7af"},"source":["#print(tokenizer.word_index.items())\n","print(embeddings_index.get().shape)"],"execution_count":47,"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-47-c10de492eabc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#print(tokenizer.word_index.items())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'a;lkjsdffds'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q_RQ15jsWgOO","executionInfo":{"status":"ok","timestamp":1633117646888,"user_tz":360,"elapsed":343,"user":{"displayName":"Scott Miner","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhxCt3KNHFc4ZlPw4tBG4aCXpKie347LZ9L1JAVuA=s64","userId":"08363991975257577328"}},"outputId":"f31b6d6f-01e2-4dd9-ec10-97f565100188"},"source":["print(embedding_matrix[1].shape[0])"],"execution_count":51,"outputs":[{"output_type":"stream","name":"stdout","text":["300\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XUnhm2oeGSTd","executionInfo":{"status":"ok","timestamp":1633117808416,"user_tz":360,"elapsed":691,"user":{"displayName":"Scott Miner","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhxCt3KNHFc4ZlPw4tBG4aCXpKie347LZ9L1JAVuA=s64","userId":"08363991975257577328"}},"outputId":"ac76ed6c-2d78-437a-c94f-b35d863d88c9"},"source":["\n","# create a weight matrix for words in training docs\n","embedding_matrix = np.zeros((size_of_vocabulary, 300))\n","hits = 0\n","misses = 0\n","\n","for word, i in tokenizer.word_index.items():\n","    embedding_vector = embeddings_index.get(word)\n","    if embedding_vector is not None and embedding_vector.shape[0] != 0:       \n","        embedding_matrix[i] = embedding_vector\n","        hits += 1\n","    else:\n","        misses += 1\n","print(f'Converted {hits} words ({misses} misses)')       \n"],"execution_count":59,"outputs":[{"output_type":"stream","name":"stdout","text":["Converted 73041 words (39162 misses)\n"]}]},{"cell_type":"code","metadata":{"id":"Jw4z5DKVUDXv"},"source":[""],"execution_count":null,"outputs":[]}]}