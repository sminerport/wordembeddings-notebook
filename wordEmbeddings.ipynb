{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"wordEmbeddings.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","mount_file_id":"1StIBevO1Ip-xSZEZkDCLXwU-PvGwXXt_","authorship_tag":"ABX9TyOSSBqc3P/fHnHHz1oMdbzH"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"r4ephzhBKM68","executionInfo":{"status":"ok","timestamp":1633104847545,"user_tz":360,"elapsed":77873,"user":{"displayName":"Scott Miner","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhxCt3KNHFc4ZlPw4tBG4aCXpKie347LZ9L1JAVuA=s64","userId":"08363991975257577328"}},"outputId":"ea7e0c5b-d32c-412d-d713-d0c6a10a7bc1"},"source":["# import libraries\n","import pandas as pd\n","import numpy as np\n","import os\n","\n","TRAIN_CSV = 'Train.csv'\n","VALID_CSV = 'Valid.csv'\n","MODEL_NAME = 'best_model.h5'\n","data_path = '/content/drive/MyDrive/Colab Notebooks/wordEmbeddings/data/movie_reviews'\n","\n","training = os.path.join(data_path, TRAIN_CSV)\n","validation = os.path.join(data_path, VALID_CSV)\n","model_path = os.path.join(data_path, MODEL_NAME)\n","# reading csv files\n","train = pd.read_csv(training)\n","valid = pd.read_csv(validation)\n","\n","#train_test split\n","x_tr, y_tr = train['text'].values, train['label'].values\n","x_val, y_val = valid['text'].values, valid['label'].values\n","\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","\n","#Tokenize the sentences\n","tokenizer = Tokenizer()\n","#preparing vocabulary\n","tokenizer.fit_on_texts(list(x_tr))\n","\n","# converting text into integer sequences\n","x_tr_seq = tokenizer.texts_to_sequences(x_tr)\n","x_val_seq = tokenizer.texts_to_sequences(x_val)\n","\n","# padding to prepare sequences of same length\n","x_tr_seq = pad_sequences(x_tr_seq, maxlen=100)\n","x_val_seq = pad_sequences(x_val_seq, maxlen=100)\n","\n","size_of_vocabulary = len(tokenizer.word_index) + 1 #+1 for padding\n","print(size_of_vocabulary)\n","\n","# build two different NLP models of the same architecture.  The first learns\n","# embeddings from scratch the second uses pretrained word embeddings\n","from keras.models import *\n","from keras.layers import *\n","from keras.callbacks import *\n","\n","model = Sequential()\n","\n","#embedding layer\n","model.add(Embedding(size_of_vocabulary,300,input_length=100,trainable=True))\n","\n","#lstm layer\n","model.add(LSTM(128,return_sequences=True,dropout=0.2))\n","\n","#Global Maxpooling\n","model.add(GlobalMaxPooling1D())\n","\n","#Dense Layer\n","model.add(Dense(64,activation='relu'))\n","model.add(Dense(1,activation='sigmoid'))\n","\n","#Add loss function, metrics, optimizer\n","model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['acc'])\n","\n","#addingcallbacks\n","es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=3)\n","mc = ModelCheckpoint(model_path, monitor='val_acc', mode='max', \n","                     save_best_only=True, verbose=1)\n","\n","#Print summary of model\n","print(model.summary())\n","\n","history = model.fit(np.array(x_tr_seq),np.array(y_tr),batch_size=128,epochs=10,\n","                    validation_data=(np.array(x_val_seq),np.array(y_val)),\n","                    verbose=1,callbacks=[es,mc])\n","\n","#loading best model\n","from keras.models import load_model\n","model = load_model(model_path)\n","\n","# evaluation\n","_,val_acc = model.evaluate(x_val_seq,y_val,batch_size=128)\n","print(val_acc)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["112204\n","Model: \"sequential\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding (Embedding)        (None, 100, 300)          33661200  \n","_________________________________________________________________\n","lstm (LSTM)                  (None, 100, 128)          219648    \n","_________________________________________________________________\n","global_max_pooling1d (Global (None, 128)               0         \n","_________________________________________________________________\n","dense (Dense)                (None, 64)                8256      \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 1)                 65        \n","=================================================================\n","Total params: 33,889,169\n","Trainable params: 33,889,169\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n","Epoch 1/10\n","313/313 [==============================] - 12s 22ms/step - loss: 0.4079 - acc: 0.8062 - val_loss: 0.3442 - val_acc: 0.8434\n","\n","Epoch 00001: val_acc improved from -inf to 0.84340, saving model to /content/drive/MyDrive/Colab Notebooks/wordEmbeddings/data/movie_reviews/best_model.h5\n","Epoch 2/10\n","313/313 [==============================] - 6s 20ms/step - loss: 0.2056 - acc: 0.9200 - val_loss: 0.3201 - val_acc: 0.8690\n","\n","Epoch 00002: val_acc improved from 0.84340 to 0.86900, saving model to /content/drive/MyDrive/Colab Notebooks/wordEmbeddings/data/movie_reviews/best_model.h5\n","Epoch 3/10\n","313/313 [==============================] - 6s 20ms/step - loss: 0.1042 - acc: 0.9634 - val_loss: 0.4145 - val_acc: 0.8502\n","\n","Epoch 00003: val_acc did not improve from 0.86900\n","Epoch 4/10\n","313/313 [==============================] - 6s 21ms/step - loss: 0.0458 - acc: 0.9853 - val_loss: 0.5443 - val_acc: 0.8540\n","\n","Epoch 00004: val_acc did not improve from 0.86900\n","Epoch 5/10\n","313/313 [==============================] - 6s 20ms/step - loss: 0.0240 - acc: 0.9926 - val_loss: 0.6544 - val_acc: 0.8458\n","\n","Epoch 00005: val_acc did not improve from 0.86900\n","Epoch 00005: early stopping\n","40/40 [==============================] - 1s 5ms/step - loss: 0.3201 - acc: 0.8690\n","0.8690000176429749\n"]}]},{"cell_type":"markdown","metadata":{"id":"TaRYpqS_XgLX"},"source":["Build version II using GloVe pretrained word embeddings. Let's load the GloVe embeddings into our environment. "]},{"cell_type":"markdown","metadata":{"id":"Y_QWZspi8Z8z"},"source":["# Download file."]},{"cell_type":"code","metadata":{"id":"KIiyDx1actOk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1633111054813,"user_tz":360,"elapsed":488278,"user":{"displayName":"Scott Miner","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhxCt3KNHFc4ZlPw4tBG4aCXpKie347LZ9L1JAVuA=s64","userId":"08363991975257577328"}},"outputId":"d17314a5-f8fb-4af3-eca3-532a5478d2f3"},"source":["import os\n","import tqdm\n","import requests\n","import zipfile\n","\n","URL = 'http://nlp.stanford.edu/data/glove.840B.300d.zip'\n","\n","def fetch_data(url=URL, target_file='/content/drive/MyDrive/Colab Notebooks/wordEmbeddings/embeddings/glove.zip', delete_zip=False):\n","    # if dataset exists exit\n","    if os.path.isfile(target_file):\n","        print('datasets already downloaded')\n","        return\n","\n","        #download (large) zip file\n","    #for large https request on stream mode to avoid out of memory issues\n","    #see : http://masnun.com/2016/09/18/python-using-the-requests-module-to-download-large-files-efficiently.html\n","    print(\"**************************\")\n","    print(\"  Downloading zip file\")\n","    print(\"  >_<  Please wait >_< \")\n","    print(\"**************************\")\n","    response = requests.get(url, stream=True)\n","    #read chunk by chunk\n","    handle = open(target_file, \"wb\")\n","    for chunk in tqdm.tqdm(response.iter_content(chunk_size=512)):\n","        if chunk:  \n","            handle.write(chunk)\n","    handle.close()  \n","    print(\"  Download completed ;) :\") \n","    #extract zip_file\n","    zf = zipfile.ZipFile(target_file)\n","    print(\"1. Extracting {} file\".format(target_file))\n","    zf.extractall(path='/content/drive/MyDrive/Colab Notebooks/wordEmbeddings/embeddings')\n","    if delete_zip:\n","        print(\"2. Deleting {} file\".format(dataset_name+\".zip\"))\n","        os.remove(path=zip_file)\n","\n","fetch_data()"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["**************************\n","  Downloading zip file\n","  >_<  Please wait >_< \n","**************************\n"]},{"output_type":"stream","name":"stderr","text":["4251502it [06:52, 10299.49it/s]\n"]},{"output_type":"stream","name":"stdout","text":["  Download completed ;) :\n","1. Extracting /content/drive/MyDrive/Colab Notebooks/wordEmbeddings/embeddings/glove.zip file\n"]}]},{"cell_type":"code","metadata":{"id":"tdRSoUEoXd64"},"source":["import tqdm\n","\n","EMBEDDING_VECTOR_LENGTH = 50 # <=200\n","def construct_embedding_matrix(glove_file, word_index):\n","    embedding_dict = {}\n","    with open(glove_file,'r') as f:\n","        for line in f:\n","            values=line.split()\n","            # get the word\n","            word=values[0]\n","            if word in word_index.keys():\n","                # get the vector\n","                vector = np.asarray(values[1:], 'float32')\n","                embedding_dict[word] = vector\n","\n","    ### oov words (out of vocabulary words) mapped to 0 vectors\n","\n","    num_words = len(word_index)+1\n","    # init to 0\n","    embedding_matrix=np.zeros((num_words, EMBEDDING_VECTOR_LENGTH))\n","\n","    for word,i in tqdm.tqdm(word_index.items()):\n","        if i < num_words:\n","            vect=embedding_dict.get(word, [])\n","            if len(vect)>0:\n","                embedding_matrix[i] = vect[:EMBEDDING_VECTOR_LENGTH]\n","    return embedding_matrix\n","\n","embedding_matrix = construct_embedding_matrix(glove_file, \n","                                              tokenizer.tokenizer.word_index)\n","\n"],"execution_count":null,"outputs":[]}]}