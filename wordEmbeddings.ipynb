{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"wordEmbeddings.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","mount_file_id":"1StIBevO1Ip-xSZEZkDCLXwU-PvGwXXt_","authorship_tag":"ABX9TyPOmDqfcw0d3f7a8+gIBm4i"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"r4ephzhBKM68","executionInfo":{"status":"ok","timestamp":1633104847545,"user_tz":360,"elapsed":77873,"user":{"displayName":"Scott Miner","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhxCt3KNHFc4ZlPw4tBG4aCXpKie347LZ9L1JAVuA=s64","userId":"08363991975257577328"}},"outputId":"ea7e0c5b-d32c-412d-d713-d0c6a10a7bc1"},"source":["# import libraries\n","import pandas as pd\n","import numpy as np\n","import os\n","\n","TRAIN_CSV = 'Train.csv'\n","VALID_CSV = 'Valid.csv'\n","MODEL_NAME = 'best_model.h5'\n","data_path = '/content/drive/MyDrive/Colab Notebooks/wordEmbeddings/data/movie_reviews'\n","\n","training = os.path.join(data_path, TRAIN_CSV)\n","validation = os.path.join(data_path, VALID_CSV)\n","model_path = os.path.join(data_path, MODEL_NAME)\n","# reading csv files\n","train = pd.read_csv(training)\n","valid = pd.read_csv(validation)\n","\n","#train_test split\n","x_tr, y_tr = train['text'].values, train['label'].values\n","x_val, y_val = valid['text'].values, valid['label'].values\n","\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","\n","#Tokenize the sentences\n","tokenizer = Tokenizer()\n","#preparing vocabulary\n","tokenizer.fit_on_texts(list(x_tr))\n","\n","# converting text into integer sequences\n","x_tr_seq = tokenizer.texts_to_sequences(x_tr)\n","x_val_seq = tokenizer.texts_to_sequences(x_val)\n","\n","# padding to prepare sequences of same length\n","x_tr_seq = pad_sequences(x_tr_seq, maxlen=100)\n","x_val_seq = pad_sequences(x_val_seq, maxlen=100)\n","\n","size_of_vocabulary = len(tokenizer.word_index) + 1 #+1 for padding\n","print(size_of_vocabulary)\n","\n","# build two different NLP models of the same architecture.  The first learns\n","# embeddings from scratch the second uses pretrained word embeddings\n","from keras.models import *\n","from keras.layers import *\n","from keras.callbacks import *\n","\n","model = Sequential()\n","\n","#embedding layer\n","model.add(Embedding(size_of_vocabulary,300,input_length=100,trainable=True))\n","\n","#lstm layer\n","model.add(LSTM(128,return_sequences=True,dropout=0.2))\n","\n","#Global Maxpooling\n","model.add(GlobalMaxPooling1D())\n","\n","#Dense Layer\n","model.add(Dense(64,activation='relu'))\n","model.add(Dense(1,activation='sigmoid'))\n","\n","#Add loss function, metrics, optimizer\n","model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['acc'])\n","\n","#addingcallbacks\n","es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=3)\n","mc = ModelCheckpoint(model_path, monitor='val_acc', mode='max', \n","                     save_best_only=True, verbose=1)\n","\n","#Print summary of model\n","print(model.summary())\n","\n","history = model.fit(np.array(x_tr_seq),np.array(y_tr),batch_size=128,epochs=10,\n","                    validation_data=(np.array(x_val_seq),np.array(y_val)),\n","                    verbose=1,callbacks=[es,mc])\n","\n","#loading best model\n","from keras.models import load_model\n","model = load_model(model_path)\n","\n","# evaluation\n","_,val_acc = model.evaluate(x_val_seq,y_val,batch_size=128)\n","print(val_acc)"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["112204\n","Model: \"sequential\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding (Embedding)        (None, 100, 300)          33661200  \n","_________________________________________________________________\n","lstm (LSTM)                  (None, 100, 128)          219648    \n","_________________________________________________________________\n","global_max_pooling1d (Global (None, 128)               0         \n","_________________________________________________________________\n","dense (Dense)                (None, 64)                8256      \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 1)                 65        \n","=================================================================\n","Total params: 33,889,169\n","Trainable params: 33,889,169\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n","Epoch 1/10\n","313/313 [==============================] - 12s 22ms/step - loss: 0.4079 - acc: 0.8062 - val_loss: 0.3442 - val_acc: 0.8434\n","\n","Epoch 00001: val_acc improved from -inf to 0.84340, saving model to /content/drive/MyDrive/Colab Notebooks/wordEmbeddings/data/movie_reviews/best_model.h5\n","Epoch 2/10\n","313/313 [==============================] - 6s 20ms/step - loss: 0.2056 - acc: 0.9200 - val_loss: 0.3201 - val_acc: 0.8690\n","\n","Epoch 00002: val_acc improved from 0.84340 to 0.86900, saving model to /content/drive/MyDrive/Colab Notebooks/wordEmbeddings/data/movie_reviews/best_model.h5\n","Epoch 3/10\n","313/313 [==============================] - 6s 20ms/step - loss: 0.1042 - acc: 0.9634 - val_loss: 0.4145 - val_acc: 0.8502\n","\n","Epoch 00003: val_acc did not improve from 0.86900\n","Epoch 4/10\n","313/313 [==============================] - 6s 21ms/step - loss: 0.0458 - acc: 0.9853 - val_loss: 0.5443 - val_acc: 0.8540\n","\n","Epoch 00004: val_acc did not improve from 0.86900\n","Epoch 5/10\n","313/313 [==============================] - 6s 20ms/step - loss: 0.0240 - acc: 0.9926 - val_loss: 0.6544 - val_acc: 0.8458\n","\n","Epoch 00005: val_acc did not improve from 0.86900\n","Epoch 00005: early stopping\n","40/40 [==============================] - 1s 5ms/step - loss: 0.3201 - acc: 0.8690\n","0.8690000176429749\n"]}]},{"cell_type":"markdown","metadata":{"id":"TaRYpqS_XgLX"},"source":["Build version II using GloVe pretrained word embeddings. Let's load the GloVe embeddings into our environment. "]},{"cell_type":"code","metadata":{"id":"KIiyDx1actOk"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tdRSoUEoXd64"},"source":["# load the whole embedding into memory\n","embeddings_index = dict()\n","f = open"],"execution_count":null,"outputs":[]}]}